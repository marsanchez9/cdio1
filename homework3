from stac_client import STACClient
from image_processor import ImageProcessor  
from git_handler import GitHandler
import time

def main():
    # CONFIGURACIÓN DEL PROCESO
    DATE_RANGE = ("2025-01-01", "2025-06-30")
    CLOUD_THRESHOLD = 0.2
    AOI = {"type": "Polygon", "coordinates": [[...]]}  # Coordenadas reales
    
    # 1. INICIALIZAR COMPONENTES
    stac_client = STACClient()
    image_processor = ImageProcessor()
    git_handler = GitHandler()
    
    # 2. BUSCAR IMÁGENES EN CATÁLOGO STAC
    print("Querying STAC API...")
    items = stac_client.search(
        date_range=DATE_RANGE,
        aoi=AOI,
        cloud_cover=CLOUD_THRESHOLD
    )
    
    # 3. DESCARGAS COMPARATIVAS (1 vs 4 HILOS)
    # Single-thread
    start_time = time.time()
    single_thread_results = stac_client.download_images(items, max_workers=1)
    single_thread_time = time.time() - start_time
    
    # Multi-thread  
    start_time = time.time()
    multi_thread_results = stac_client.download_images(items, max_workers=4)
    multi_thread_time = time.time() - start_time
    
    download_metrics = {
        'single_thread': single_thread_time,
        'multi_thread': multi_thread_time,
        'speedup': single_thread_time / multi_thread_time
    }
    
    # 4. FILTRADO DE NUBES
    clean_images = image_processor.filter_cloudy_images(multi_thread_results)
    
    # 5. CÁLCULO NDWI COMPARATIVO (1 vs 4 PROCESOS)
    # Single-process
    start_time = time.time()
    single_process_ndwi = image_processor.calculate_ndwi_batch(clean_images, processes=1)
    single_process_time = time.time() - start_time
    
    # Multi-process
    start_time = time.time()
    multi_process_ndwi = image_processor.calculate_ndwi_batch(clean_images, processes=4)  
    multi_process_time = time.time() - start_time
    
    ndwi_metrics = {
        'single_process': single_process_time,
        'multi_process': multi_process_time, 
        'speedup': single_process_time / multi_process_time
    }
    
    # 6. PUBLICAR RESULTADOS
    git_handler.push_results(
        ndwi_results=multi_process_ndwi,
        download_metrics=download_metrics,
        ndwi_metrics=ndwi_metrics
    )
    
    print(f"Download speedup: {download_metrics['speedup']:.2f}x")
    print(f"NDWI calculation speedup: {ndwi_metrics['speedup']:.2f}x")

if __name__ == "__main__":
    main()







22222222

import requests
import rasterio
from pystac_client import Client
import concurrent.futures

class STACClient:
    def __init__(self):
        self.client = Client.open("https://earth-search.aws.element84.com/v1")
    
    def search(self, date_range, aoi, cloud_cover):
        # STAC search implementation
        pass
    
    def download_images(self, items, threads=1):
        if threads == 1:
            return [self._download_single(item) for item in items]
        else:
            with ThreadPoolExecutor(max_workers=threads) as executor:
                return list(executor.map(self._download_single, items))
    
    def _download_single(self, item):
        # Download individual image
        pass



33333333333333




import requests
from pystac_client import Client
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import rasterio
from urllib.parse import urlparse

class STACClient:
    def __init__(self, stac_api_url="https://earth-search.aws.element84.com/v1"):
        self.client = Client.open(stac_api_url)
        self.download_dir = "downloaded_images"
        os.makedirs(self.download_dir, exist_ok=True)
    
    def search(self, date_range, aoi, cloud_cover=0.2, collections=["sentinel-2-l2a"]):
        """
        Busca imágenes en el catálogo STAC según criterios
        
        Args:
            date_range: Tupla (start_date, end_date)
            aoi: GeoJSON con área de interés
            cloud_cover: Máximo porcentaje de nubes permitido
            collections: Colecciones de satélites a buscar
        """
        search_results = self.client.search(
            collections=collections,
            intersects=aoi,
            datetime=f"{date_range[0]}/{date_range[1]}",
            query={"cloud_cover": {"lt": cloud_cover * 100}}  # STAC usa porcentaje 0-100
        )
        
        items = list(search_results.items())
        print(f"Found {len(items)} items matching criteria")
        return items
    
    def _download_asset(self, item, asset_key="visual"):
        """
        Descarga un asset específico de un ítem STAC
        
        Args:
            item: Ítem STAC con metadatos
            asset_key: Tipo de asset a descargar ('visual', 'B04', 'B08', etc.)
        """
        try:
            asset = item.assets.get(asset_key)
            if not asset:
                print(f"Asset {asset_key} not found in item {item.id}")
                return None
            
            # Descargar el archivo
            response = requests.get(asset.href, stream=True)
            response.raise_for_status()
            
            # Guardar localmente
            filename = os.path.join(self.download_dir, f"{item.id}_{asset_key}.tif")
            with open(filename, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"Downloaded: {filename}")
            return filename
            
        except Exception as e:
            print(f"Error downloading {item.id}: {e}")
            return None
    
    def download_images(self, items, max_workers=4, asset_keys=["B04", "B08"]):
        """
        Descarga imágenes en paralelo usando ThreadPoolExecutor
        
        Args:
            items: Lista de ítems STAC a descargar
            max_workers: Número de hilos para descargas paralelas
            asset_keys: Bandas espectrales a descargar
        """
        downloaded_files = []
        
        def download_task(item_asset):
            item, asset_key = item_asset
            return self._download_asset(item, asset_key)
        
        # Crear todas las tareas (item, asset_key)
        tasks = []
        for item in items:
            for asset_key in asset_keys:
                tasks.append((item, asset_key))
        
        # Ejecutar descargas en paralelo
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_task = {executor.submit(download_task, task): task for task in tasks}
            
            for future in as_completed(future_to_task):
                result = future.result()
                if result:
                    downloaded_files.append(result)
        
        print(f"Downloaded {len(downloaded_files)} files using {max_workers} workers")
        return downloaded_files










444444444444








import rasterio
import numpy as np
from concurrent.futures import ProcessPoolExecutor
import os
from rasterio.enums import Resampling

class ImageProcessor:
    def __init__(self, output_dir="ndwi_results"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
    
    def filter_cloudy_images(self, image_paths, cloud_threshold=0.3):
        """
        Filtra imágenes basándose en cobertura de nubes
        
        Args:
            image_paths: Lista de rutas a imágenes
            cloud_threshold: Porcentaje máximo de nubes permitido
        """
        clean_images = []
        
        for image_path in image_paths:
            try:
                # En Sentinel-2, la banda SCL (Scene Classification) tiene info de nubes
                cloud_coverage = self._calculate_cloud_coverage(image_path)
                
                if cloud_coverage <= cloud_threshold:
                    clean_images.append(image_path)
                    print(f"Clean image: {os.path.basename(image_path)} - clouds: {cloud_coverage:.2%}")
                else:
                    print(f"Cloudy image rejected: {os.path.basename(image_path)} - clouds: {cloud_coverage:.2%}")
                    
            except Exception as e:
                print(f"Error processing {image_path}: {e}")
        
        print(f"Filtered {len(clean_images)} clean images from {len(image_paths)} total")
        return clean_images
    
    def _calculate_cloud_coverage(self, image_path):
        """
        Calcula el porcentaje de nubes en una imagen usando la banda de clasificación
        """
        try:
            # Para Sentinel-2, banda SCL (Scene Classification Layer)
            # Valores 3: nubes sombra, 8-9-10: nubes medias/altas/opacas
            with rasterio.open(image_path) as src:
                if 'SCL' in src.descriptions:
                    scl_band_idx = [i for i, desc in enumerate(src.descriptions) if desc == 'SCL'][0]
                    scl_data = src.read(scl_band_idx + 1)  # +1 porque rasterio indexa desde 1
                    
                    cloud_pixels = np.sum((scl_data >= 3) & (scl_data <= 10))
                    total_pixels = scl_data.size
                    
                    return cloud_pixels / total_pixels
                else:
                    # Si no hay banda SCL, estimación simple basada en reflectancia
                    return self._estimate_clouds_from_reflectance(image_path)
                    
        except Exception as e:
            print(f"Cloud detection error for {image_path}: {e}")
            return 0.5  # Valor conservador si hay error
    
    def _estimate_clouds_from_reflectance(self, image_path):
        """
        Estimación simple de nubes basada en reflectancia en banda azul
        """
        try:
            with rasterio.open(image_path) as src:
                # Banda azul (B02) generalmente tiene alta reflectancia en nubes
                blue_band = src.read(1)  # Ajustar índice según sensor
                
                # Umbral empírico para detectar nubes
                cloud_threshold = 0.3
                cloud_pixels = np.sum(blue_band > cloud_threshold)
                total_pixels = blue_band.size
                
                return cloud_pixels / total_pixels
                
        except:
            return 0.0
    
    def _calculate_single_ndwi(self, green_path, nir_path):
        """
        Calcula NDWI para un par de bandas verde y NIR
        
        NDWI = (Green - NIR) / (Green + NIR)
        """
        try:
            with rasterio.open(green_path) as green_src:
                green_data = green_src.read(1).astype(np.float32)
                profile = green_src.profile
            
            with rasterio.open(nir_path) as nir_src:
                nir_data = nir_src.read(1).astype(np.float32)
            
            # Cálculo NDWI con estabilidad numérica
            denominator = green_data + nir_data
            denominator[denominator == 0] = 1e-8  # Evitar división por cero
            
            ndwi = (green_data - nir_data) / denominator
            
            # Guardar resultado
            output_filename = f"ndwi_{os.path.basename(green_path).split('_')[0]}.tif"
            output_path = os.path.join(self.output_dir, output_filename)
            
            profile.update({
                'dtype': rasterio.float32,
                'count': 1,
                'compress': 'lzw'
            })
            
            with rasterio.open(output_path, 'w', **profile) as dst:
                dst.write(ndwi.astype(rasterio.float32), 1)
            
            print(f"NDWI calculated: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"Error calculating NDWI for {green_path}: {e}")
            return None
    
    def calculate_ndwi_batch(self, image_paths, processes=4):
        """
        Calcula NDWI para lote de imágenes usando multiprocessing
        
        Args:
            image_paths: Lista de todas las bandas descargadas
            processes: Número de procesos paralelos
        """
        # Agrupar bandas verde y NIR por escena
        green_bands = [p for p in image_paths if 'B03' in p or 'green' in p]
        nir_bands = [p for p in image_paths if 'B08' in p or 'nir' in p]
        
        # Emparejar bandas por escena
        paired_bands = []
        for green in green_bands:
            scene_id = green.split('_')[0]
            matching_nir = [n for n in nir_bands if scene_id in n]
            if matching_nir:
                paired_bands.append((green, matching_nir[0]))
        
        print(f"Processing {len(paired_bands)} image pairs with {processes} processes")
        
        if processes == 1:
            # Procesamiento secuencial
            results = [self._calculate_single_ndwi(green, nir) for green, nir in paired_bands]
        else:
            # Procesamiento paralelo
            with ProcessPoolExecutor(max_workers=processes) as executor:
                results = list(executor.map(
                    lambda pair: self._calculate_single_ndwi(*pair), 
                    paired_bands
                ))
        
        # Filtrar resultados exitosos
        successful_results = [r for r in results if r is not None]
        print(f"Successfully processed {len(successful_results)} NDWI images")
        
        return successful_results






5555555555555555




import git
import json
import os
from datetime import datetime
import shutil

class GitHandler:
    def __init__(self, repo_path=".", remote_url=None):
        self.repo_path = repo_path
        
        try:
            self.repo = git.Repo(repo_path)
            print(f"Found existing repo at {repo_path}")
        except git.exc.InvalidGitRepositoryError:
            print("Initializing new git repository...")
            self.repo = git.Repo.init(repo_path)
            
            if remote_url:
                self.repo.create_remote('origin', remote_url)
    
    def push_results(self, ndwi_results, download_metrics, ndwi_metrics, commit_message=None):
        """
        Guarda resultados en repositorio Git y pushea a remoto
        
        Args:
            ndwi_results: Lista de archivos NDWI generados
            download_metrics: Métricas de descarga
            ndwi_metrics: Métricas de procesamiento NDWI
            commit_message: Mensaje personalizado para commit
        """
        if not commit_message:
            commit_message = f"NDWI Results {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        try:
            # 1. Guardar métricas en archivo JSON
            metrics_data = {
                'timestamp': datetime.now().isoformat(),
                'download_performance': download_metrics,
                'ndwi_performance': ndwi_metrics,
                'processed_images': len(ndwi_results),
                'image_list': [os.path.basename(p) for p in ndwi_results]
            }
            
            metrics_path = os.path.join(self.repo_path, "processing_metrics.json")
            with open(metrics_path, 'w') as f:
                json.dump(metrics_data, f, indent=2)
            
            # 2. Crear README con resumen
            readme_content = self._generate_readme(metrics_data)
            with open(os.path.join(self.repo_path, "README.md"), 'w') as f:
                f.write(readme_content)
            
            # 3. Añadir todos los archivos nuevos al staging
            self.repo.git.add('--all')
            
            # 4. Hacer commit
            self.repo.index.commit(commit_message)
            print(f"Committed: {commit_message}")
            
            # 5. Push a remoto si existe
            if hasattr(self.repo.remotes, 'origin'):
                origin = self.repo.remotes.origin
                origin.push()
                print("Pushed to remote repository")
            else:
                print("No remote repository configured - changes committed locally")
                
        except Exception as e:
            print(f"Error in Git operations: {e}")
            raise
    
    def _generate_readme(self, metrics_data):
        """
        Genera archivo README con resumen de resultados
        """
        download_speedup = metrics_data['download_performance']['speedup']
        ndwi_speedup = metrics_data['ndwi_performance']['speedup']
        
        return f"""# NDWI Processing Results

## Summary
- **Processed Images**: {metrics_data['processed_images']}
- **Processing Date**: {metrics_data['timestamp']}

## Performance Metrics
- **Download Speedup (4 threads)**: {download_speedup:.2f}x
- **NDWI Calculation Speedup (4 processes)**: {ndwi_speedup:.2f}x

## Files
- `processing_metrics.json`: Detailed performance metrics
- `ndwi_*.tif`: NDWI GeoTIFF results
- `downloaded_images/`: Original satellite imagery

## Methodology
- **Data Source**: Sentinel-2 via STAC API
- **Cloud Filtering**: Scene Classification Layer (SCL)
- **NDWI Formula**: (Green - NIR) / (Green + NIR)
- **Parallel Processing**: ThreadPoolExecutor (downloads), ProcessPoolExecutor (NDWI)
"""
