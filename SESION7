import os   #para trabjar con archivos, carpetas..
import rasterio
import numpy as np       #para trabajar con arreys numéricos
from scipy.ndimage import convolve #Importa la función convolve de SciPy para aplicar filtros/convoluciones a imagenes
import geopandas as gpd
from shapely.geometry import Point, shape
from datetime import datetime
import pandas as pd
from scipy import ndimage
from rasterio.mask import mask
import json
import math

# ===== FUNCIÓN PARA CALCULAR RMSE =====
def calculate_rmse(coords_estimated, coords_ground_truth):
    """
    Calcula el RMSE entre coordenadas estimadas y ground truth
    
    Args:
        coords_estimated: lista de tuplas (x, y) estimadas
        coords_ground_truth: lista de tuplas (x, y) de ground truth
    
    Returns:
        rmse: valor RMSE en metros
    """
    if len(coords_estimated) == 0 or len(coords_ground_truth) == 0:
        return float('inf')
    
    # Convertir a arrays numpy para cálculos más eficientes
    est_array = np.array(coords_estimated)
    gt_array = np.array(coords_ground_truth)
    
    # Calcular distancia euclidiana para cada punto
    distances = np.sqrt(np.sum((est_array - gt_array) ** 2, axis=1))
    
    # RMSE es la raíz cuadrada del promedio de los cuadrados de las distancias
    rmse = np.sqrt(np.mean(distances ** 2))
    
    return rmse

# =====  FUNCIÓN PARA CARGAR GROUND TRUTH =====
def load_ground_truth(gt_file_path):
    """
    Carga datos de ground truth desde archivo CSV
    
    Args:
        gt_file_path: ruta al archivo CSV de ground truth
    
    Returns:
        lista de tuplas (x, y) con coordenadas UTM
    """
    try:
        df_gt = pd.read_csv(gt_file_path)
        # Asumimos que las columnas se llaman 'X' y 'Y' (coordenadas UTM)
        coords = list(zip(df_gt['X'], df_gt['Y']))
        print(f"  → Cargados {len(coords)} puntos de ground truth desde {gt_file_path}")
        return coords
    except Exception as e:
        print(f"  ⚠️ Error cargando ground truth: {e}")
        return []

# =====  FUNCIÓN PARA ENCONTRAR COINCIDENCIAS MÁS CERCANAS =====
def find_nearest_matches(coords_estimated, coords_ground_truth, max_distance=100):
    """
    Encuentra los puntos más cercanos entre estimaciones y ground truth
    
    Args:
        coords_estimated: coordenadas estimadas
        coords_ground_truth: coordenadas de ground truth
        max_distance: distancia máxima para considerar coincidencia (metros)
    
    Returns:
        matched_est: coordenadas estimadas que tienen match
        matched_gt: coordenadas ground truth correspondientes
    """
    matched_est = []
    matched_gt = []
    
    for gt_point in coords_ground_truth:
        min_distance = float('inf')
        nearest_est_point = None
        
        for est_point in coords_estimated:
            distance = math.sqrt((gt_point[0] - est_point[0])**2 + (gt_point[1] - est_point[1])**2)
            if distance < min_distance and distance < max_distance:
                min_distance = distance
                nearest_est_point = est_point
        
        if nearest_est_point is not None:
            matched_est.append(nearest_est_point)
            matched_gt.append(gt_point)
    
    print(f"  → Encontradas {len(matched_est)} coincidencias de {len(coords_ground_truth)} puntos GT")
    return matched_est, matched_gt

# ===== NUEVO: FUNCIONES PARA EROSIÓN/ACRECIÓN =====
def calculate_erosion_accretion(coastline_dates):
    """
    Calcula erosión y acreción entre líneas de costa de diferentes fechas
    
    Args:
        coastline_dates: diccionario con {fecha: lista de coordenadas}
    
    Returns:
        DataFrame con resultados de erosión/acreción
    """
    print("\n" + "="*60)
    print("CÁLCULO DE EROSIÓN/ACRECIÓN PARA 2025")
    print("="*60)
    
    # Filtrar solo las fechas de la primera mitad de 2025
    dates_2025 = {date: coords for date, coords in coastline_dates.items() if date.startswith('2025-')}
    
    if len(dates_2025) < 2:
        print("❌ No hay suficientes fechas de 2025 para calcular erosión/acreción")
        return pd.DataFrame()
    
    print(f"→ Encontradas {len(dates_2025)} fechas de 2025")
    
    results = []
    
    # Ordenar las fechas cronológicamente
    sorted_dates = sorted(dates_2025.keys())
    
    # Tomar la primera fecha como referencia
    reference_date = sorted_dates[0]
    reference_coords = dates_2025[reference_date]
    
    print(f"→ Usando {reference_date} como línea de costa de referencia")
    
    # Comparar cada fecha posterior con la referencia
    for current_date in sorted_dates[1:]:
        print(f"→ Comparando {current_date} con referencia {reference_date}")
        
        current_coords = dates_2025[current_date]
        
        if not reference_coords or not current_coords:
            print(f"  ⚠️  No hay coordenadas para {current_date}, saltando...")
            continue
        
        # Calcular cambios usando el método de transectos
        erosion, accretion, total_change = calculate_coastline_changes(reference_coords, current_coords)
        
        results.append({
            'reference_date': reference_date,
            'current_date': current_date,
            'days_difference': (datetime.strptime(current_date, '%Y-%m-%d') - 
                              datetime.strptime(reference_date, '%Y-%m-%d')).days,
            'erosion_meters': erosion,
            'accretion_meters': accretion,
            'net_change_meters': total_change,
            'erosion_rate_meters_day': erosion / max(1, (datetime.strptime(current_date, '%Y-%m-%d') - 
                                                       datetime.strptime(reference_date, '%Y-%m-%d')).days),
            'accretion_rate_meters_day': accretion / max(1, (datetime.strptime(current_date, '%Y-%m-%d') - 
                                                           datetime.strptime(reference_date, '%Y-%m-%d')).days)
        })
        
        print(f"  → Erosión: {erosion:.2f}m, Acreción: {accretion:.2f}m, Cambio neto: {total_change:.2f}m")
    
    return pd.DataFrame(results)

def calculate_coastline_changes(reference_coords, current_coords, transect_spacing=10, transect_length=100):
    """
    Calcula cambios en la línea de costa usando el método de transectos
    
    Args:
        reference_coords: coordenadas de la línea de referencia
        current_coords: coordenadas de la línea actual
        transect_spacing: separación entre transectos (metros)
        transect_length: longitud de cada transecto (metros)
    
    Returns:
        erosión_total, acreción_total, cambio_neto
    """
    # Este es un método simplificado - en la práctica necesitarías:
    # 1. Ordenar puntos de la costa
    # 2. Crear transectos perpendiculares
    # 3. Calcular intersecciones
    # 4. Medir distancias
    
    # Por ahora, usamos una aproximación con distancias mínimas
    erosion_total = 0
    accretion_total = 0
    points_analyzed = 0
    
    for ref_point in reference_coords[:100]:  # Limitar para eficiencia
        # Encontrar el punto más cercano en la costa actual
        min_distance = float('inf')
        nearest_point = None
        
        for curr_point in current_coords:
            distance = math.sqrt((ref_point[0]-curr_point[0])**2 + (ref_point[1]-curr_point[1])**2)
            if distance < min_distance and distance < 50:  # Límite de 50m
                min_distance = distance
                nearest_point = curr_point
        
        if nearest_point:
            # Determinar si es erosión o acreción basado en la posición relativa
            # (Esto es una simplificación - en realidad necesitas la dirección de la costa)
            if min_distance > 2:  # Ignorar cambios muy pequeños
                # Aquí deberías calcular la dirección perpendicular a la costa
                # Por simplicidad, asumimos cambios positivos como acreción
                if min_distance < 10:  # Umbral arbitrario
                    accretion_total += min_distance
                else:
                    erosion_total += min_distance
                points_analyzed += 1
    
    if points_analyzed > 0:
        erosion_avg = erosion_total / points_analyzed
        accretion_avg = accretion_total / points_analyzed
    else:
        erosion_avg = accretion_avg = 0
    
    net_change = accretion_avg - erosion_avg
    
    return erosion_avg, accretion_avg, net_change

# ===== NUEVO: FUNCIONES PARA MÉTRICAS DE VALIDACIÓN =====
def calculate_validation_metrics(all_coordinates_df, validation_results_df, cloud_ratios=None):
    """
    Calcula métricas detalladas de validación del proyecto
    
    Args:
        all_coordinates_df: DataFrame con todas las coordenadas procesadas
        validation_results_df: DataFrame con resultados de validación RMSE
        cloud_ratios: diccionario con {fecha: ratio_nubes} (opcional)
    
    Returns:
        Diccionario con todas las métricas calculadas
    """
    print("\n" + "="*60)
    print("MÉTRICAS DETALLADAS DE VALIDACIÓN")
    print("="*60)
    
    metrics = {}
    
    # 1. NÚMERO DE PUNTOS ANALIZADOS
    metrics['total_points_analyzed'] = len(all_coordinates_df)
    metrics['unique_dates'] = all_coordinates_df['Date'].nunique()
    
    print(f"1. PUNTOS ANALIZADOS:")
    print(f"   → Total puntos: {metrics['total_points_analyzed']:,}")
    print(f"   → Fechas únicas: {metrics['unique_dates']}")
    
    # 2. PUNTOS VÁLIDOS VS TOTALES
    if not validation_results_df.empty:
        total_gt_points = validation_results_df['total_gt_points'].sum()
        matched_points = validation_results_df['matched_points'].sum()
        metrics['valid_data_points'] = matched_points
        metrics['data_quality_ratio'] = matched_points / total_gt_points if total_gt_points > 0 else 0
        
        print(f"2. CALIDAD DE DATOS:")
        print(f"   → Puntos GT totales: {total_gt_points:,}")
        print(f"   → Puntos válidos (match): {matched_points:,}")
        print(f"   → Ratio calidad: {metrics['data_quality_ratio']:.1%}")
    
    # 3. ESPACIADO PROMEDIO ENTRE PUNTOS
    if not all_coordinates_df.empty:
        avg_spacing = calculate_average_spacing(all_coordinates_df)
        metrics['average_point_spacing_m'] = avg_spacing
        print(f"3. ESPACIADO ENTRE PUNTOS:")
        print(f"   → Distancia promedio: {avg_spacing:.2f} metros")
    
    # 4. PERIODO DE ANÁLISIS Y RESOLUCIÓN TEMPORAL
    if not all_coordinates_df.empty:
        time_metrics = calculate_temporal_metrics(all_coordinates_df)
        metrics.update(time_metrics)
        
        print(f"4. ANÁLISIS TEMPORAL:")
        print(f"   → Periodo: {time_metrics['timespan_days']} días")
        print(f"   → Resolución: {time_metrics['temporal_resolution_days']:.1f} días entre fechas")
        print(f"   → Primera fecha: {time_metrics['first_date']}")
        print(f"   → Última fecha: {time_metrics['last_date']}")
    
    # 5. BRECHAS POR NUBES
    if cloud_ratios:
        cloud_metrics = calculate_cloud_metrics(cloud_ratios)
        metrics.update(cloud_metrics)
        
        print(f"5. ANÁLISIS DE NUBES:")
        print(f"   → Fechas con nubes: {cloud_metrics['cloudy_dates']}")
        print(f"   → Ratio promedio nubes: {cloud_metrics['average_cloud_ratio']:.1%}")
        print(f"   → Fechas perdidas: {cloud_metrics['lost_dates']}")
    
    # 6. EROSIÓN/ACRECIÓN PROMEDIO
    if not validation_results_df.empty and 'rmse' in validation_results_df.columns:
        avg_rmse = validation_results_df['rmse'].mean()
        metrics['average_rmse'] = avg_rmse
        
        print(f"6. PRECISIÓN:")
        print(f"   → RMSE promedio: {avg_rmse:.2f} metros")
        print(f"   → Fechas con RMSE < 50m: {len([x for x in validation_results_df['rmse'] if x < 50])}")
    
    return metrics

def calculate_average_spacing(coordinates_df):
    """
    Calcula el espaciado promedio entre puntos de costa para una fecha
    """
    try:
        # Tomar una fecha como ejemplo
        sample_date = coordinates_df['Date'].iloc[0]
        date_coords = coordinates_df[coordinates_df['Date'] == sample_date]
        
        if len(date_coords) < 2:
            return 0
        
        # Calcular distancias entre puntos consecutivos
        total_distance = 0
        count = 0
        
        coords_list = list(zip(date_coords['X'], date_coords['Y']))
        
        for i in range(len(coords_list) - 1):
            x1, y1 = coords_list[i]
            x2, y2 = coords_list[i + 1]
            distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)
            total_distance += distance
            count += 1
        
        return total_distance / count if count > 0 else 0
        
    except Exception as e:
        print(f"  ⚠️  Error calculando espaciado: {e}")
        return 0

def calculate_temporal_metrics(coordinates_df):
    """
    Calcula métricas temporales del análisis
    """
    try:
        dates = pd.to_datetime(coordinates_df['Date']).sort_values()
        
        if len(dates) < 2:
            return {
                'timespan_days': 0,
                'temporal_resolution_days': 0,
                'first_date': dates.iloc[0].strftime('%Y-%m-%d') if len(dates) > 0 else 'N/A',
                'last_date': dates.iloc[-1].strftime('%Y-%m-%d') if len(dates) > 0 else 'N/A'
            }
        
        timespan = (dates.iloc[-1] - dates.iloc[0]).days
        
        # Calcular resolución temporal promedio
        date_differences = []
        for i in range(len(dates) - 1):
            diff = (dates.iloc[i + 1] - dates.iloc[i]).days
            date_differences.append(diff)
        
        avg_resolution = sum(date_differences) / len(date_differences) if date_differences else 0
        
        return {
            'timespan_days': timespan,
            'temporal_resolution_days': avg_resolution,
            'first_date': dates.iloc[0].strftime('%Y-%m-%d'),
            'last_date': dates.iloc[-1].strftime('%Y-%m-%d')
        }
        
    except Exception as e:
        print(f"  ⚠️  Error calculando métricas temporales: {e}")
        return {'timespan_days': 0, 'temporal_resolution_days': 0, 'first_date': 'N/A', 'last_date': 'N/A'}

def calculate_cloud_metrics(cloud_ratios):
    """
    Calcula métricas relacionadas con cobertura de nubes
    """
    try:
        cloudy_dates = sum(1 for ratio in cloud_ratios.values() if ratio > 0.1)  # >10% nubes
        average_ratio = sum(cloud_ratios.values()) / len(cloud_ratios) if cloud_ratios else 0
        lost_dates = sum(1 for ratio in cloud_ratios.values() if ratio > 0.5)  # >50% nubes (fechas perdidas)
        
        return {
            'cloudy_dates': cloudy_dates,
            'average_cloud_ratio': average_ratio,
            'lost_dates': lost_dates
        }
        
    except Exception as e:
        print(f"  ⚠️  Error calculando métricas de nubes: {e}")
        return {'cloudy_dates': 0, 'average_cloud_ratio': 0, 'lost_dates': 0}
    


# Carpeta on tens les imatges descarregades
base_folder = "./sentinel2_images4"
polygon_geojson_path = "/.polygon.geojson1" #ns que poner de nombre 

# ===== NUEVO: CONFIGURACIÓN GROUND TRUTH =====
ground_truth_dates = {
    '2017-05-23': './ground_truth/gt_2017-05-23.csv',
    '2019-05-23': './ground_truth/gt_2019-05-23.csv',  
    '2021-06-11': './ground_truth/gt_2021-06-11.csv'   
}
validation_results = []

# Cargar polígono del GeoJSON
with open(polygon_geojson_path) as f:
    geojson_data = json.load(f)
polygon_geom = [shape(geojson_data["features"][0]["geometry"])]  # en lista, para rasterio.mask

# Kernel per detectar canvis als veïns (3x3 excloent el centre), nos permite detectar lo bordes. Lo multiplicas con matriz 3x3 desde que se detecta
# un agua, sabiendo que agua = 1 y no agua = 0.Esta matriz se va deslizando por la imagene hasta detectar toda la linea de costa. cada número 
#representa un pixel, y los de alrededor son sus píxeles vecinos.
kernel = np.array([[1, 1, 1],
                   [1, 0, 1],
                   [1, 1, 1]])

#Ahora para calcular el kernel utilizas solo cuatro vecinos, por lo que será menos preciso, al considerar menos píxeles.
kernel_4 = np.array([[0, 1, 0],
                     [1, 0, 1],
                     [0, 1, 0]])

# Lista para guardar todas las coordenadas de todas las fechas
all_coordinates = []

# Recorre totes les carpetes d’imatges
for root, dirs, files in os.walk(base_folder):   #root = ruta de la carpeta; dirs = lista de subcarpetas; files = lista de archivos en la carpeta
    if "waterbody.tif" in files:    #verifica que exista la carpeta
        waterbody_path = os.path.join(root, "waterbody.tif")    #construye la ruta al archivo
        print(f"Processant {waterbody_path}...")    

        #breakpoint()
        # Obre i llegeix el GeoTIFF
        with rasterio.open(waterbody_path) as src:  #abre el archivo en modo lectura
            out_image, out_transform = mask(src, polygon_geom, crop=True) #sirve para recortar la imagen
            waterbody = src.read(1) #coge la primera banda que tenga agua sabiendo que agua = 1; no agua = 0. 
            profile = src.profile
            transform = src.transform  # Guardamos la transformación para coordenadas, cambia píxeles a coordenadas reales

            profile.update({
                "height": out_image.shape[1],
                "width": out_image.shape[2],
                "transform": out_transform
            }) #sirve para actualizar los metadatos de las imagenes para poder recortarlo y así determinar un AOI más pequeño



        # CORNER CASE 1: Excluir bordes del AOI (3 píxeles de cada lado), es decir de la parte de AOI elimina tres pixeles para que no
        #los confunda con tierra, simplemente no existen
        height, width = waterbody.shape
        border_mask = np.ones_like(waterbody, dtype=bool)
        border_mask[:3, :] = False        # Borde superior
        border_mask[-3:, :] = False       # Borde inferior  
        border_mask[:, :3] = False        # Borde izquierdo
        border_mask[:, -3:] = False       # Borde derecho
        
         # CORNER CASE 2: Detectar áreas nubladas (patrones irregulares)
        
        matriz_zonas_agua, num_water_components = ndimage.label(waterbody == 1) #Encuentra zonas de agua, asigna un número único a cada zona de agua
        water_component_sizes = np.bincount(matriz_zonas_agua.ravel()) #.ravel transforma la matriz en vector, 
                                                                        #bincount cuanta el numero de pixeles en cada zona(posicion 0 del vector son todos los pixeles de tierra)
        
        cloud_mask = np.ones_like(waterbody, dtype=bool) 
                                                       
        
        small_components = 0
        for label in range(1, num_water_components + 1): #recorre todas las zonas de agua, empezando en la 1, ya que la 0 es tierra
            if water_component_sizes[label] < 50:
                small_components += 1
                cloud_mask[matriz_zonas_agua == label] = False #Marca como probable nube todos los píxeles de esa mancha pequeña y los ignora
        
        cloud_ratio = small_components / max(1, num_water_components) #mira la proporcion de cuantas zonas son nubes en total
        if cloud_ratio > 0.3:
            print(f" Imagen posiblemente nublada ({cloud_ratio:.1%} componentes pequeños)")
        
        # Detecta línia de costa
        neighbor_sum = convolve(waterbody, kernel, mode='constant', cval=0) #Lo multiplica por el kernel y si los vecinos que son agua son <8 
                                                                            #es una línea de costa, sino es mar.
        coastline = np.logical_and(waterbody == 1, neighbor_sum < 8).astype(np.uint8) #np.logical_and: combina ambas condiciones → píxeles de agua con al menos un vecino de tierra.
                                                                                    #.astype(np.uint8): convierte el resultado booleano a 0s y 1s,es decir, es o no es costa.

        # Método 4-vecinos 
        neighbor_sum_4 = convolve(waterbody, kernel_4, mode='constant', cval=0)
        coastline_4 = np.logical_and(waterbody == 1, neighbor_sum_4 < 4).astype(np.uint8)
        
        # Comparar resultados entre los kernel
        pixels_8 = np.sum(coastline)
        pixels_4 = np.sum(coastline_4)
        
        print(f"   Comparación métodos:")
        print(f"     - 8-vecinos: {pixels_8} píxeles de costa")
        print(f"     - 4-vecinos: {pixels_4} píxeles de costa")
        print(f"     - Diferencia: {abs(pixels_8 - pixels_4)} píxeles")
        
        
        coastline = np.logical_and(coastline, border_mask).astype(np.uint8) #aplicar la eliminacion de los bordes AOI
        coastline = np.logical_and(coastline, cloud_mask).astype(np.uint8)  #aplicar lo de las nubes
        
        # Actualitza perfil per guardar
        profile.update(dtype=rasterio.uint8, count=1) #actualiza la forma de guardar los datos, por un lado count = 1 crea una sola banda(la de costa)
                                                        #por otro lado, dtype=rasterio.uint8 lo hace solo para ocupar menos espacio, ya que usamos 1 y 0 no floats

        # Ruta de sortida
        coastline_path = os.path.join(root, "coastline.tif") #ruta de salida para el archivo "coastline.tif" en la misma carpeta.

        # Guarda resultados en un nuevo archivo
        with rasterio.open(coastline_path, 'w', **profile) as dst: 
            dst.write(coastline, 1)     #crea un nuevo archivo y escribe la coastline

        print(f"  → Línia de costa guardada a {coastline_path}")
        
        # 🆕 EXPORTAR A GEOJSON
        print("  → Exportant coordenades a GeoJSON...")
        
        # Troba els píxels que són costa (valors = 1)
        y_indices, x_indices = np.where(coastline == 1) #np.where busca todos las coordenadas de los píxeles coastline
        
        # Convierte los pixeles a coordenadas XY UTM Z31
        coordinates = []
        for x_pixel, y_pixel in zip(x_indices, y_indices):
            # Transforma coordenades de píxel a coordenadas UTM
            x_coord, y_coord = transform * (x_pixel, y_pixel)
            coordinates.append((x_coord, y_coord))
        
        # =====  VALIDACIÓN CON GROUND TRUTH =====
        folder_name = os.path.basename(root)
        current_date = folder_name
        
        print(f"  → Validando con ground truth para: {current_date}")
        
        # Buscar ground truth correspondiente
        matched_gt_date = None
        for gt_date in ground_truth_dates.keys():
            if gt_date[:7] == current_date[:7]:  # Comparar YYYY-MM
                matched_gt_date = gt_date
                break
        
        if matched_gt_date and os.path.exists(ground_truth_dates[matched_gt_date]):
            gt_coords = load_ground_truth(ground_truth_dates[matched_gt_date])
            
            if gt_coords:
                matched_est, matched_gt = find_nearest_matches(coordinates, gt_coords)
                
                if matched_est and matched_gt:
                    rmse = calculate_rmse(matched_est, matched_gt)
                    
                    validation_results.append({
                        'image_date': current_date,
                        'gt_date': matched_gt_date,
                        'rmse': rmse,
                        'matched_points': len(matched_est),
                        'total_gt_points': len(gt_coords)
                    })
                    
                    print(f"  ✅ RMSE: {rmse:.2f} metros")
                    if rmse < 50:
                        print(f"  🎯 OBJETIVO CUMPLIDO: RMSE < 50m")
                    else:
                        print(f"  ⚠️  OBJETIVO NO CUMPLIDO: RMSE > 50m")
        
        # 🆕 GUARDAR PARA CSV GLOBAL
        # Extraer fecha del nombre de carpeta o usar fecha actual
        folder_name = os.path.basename(root)
        
        # Intentar extraer fecha del nombre de carpeta
        try:
            # Si la carpeta tiene formato de fecha: "2024-01-15"
            date_str = folder_name
        except:
            # Si no, usar fecha de procesamiento
            date_str = datetime.now().strftime("%Y-%m-%d")
        
        # Añadir todas las coordenadas con su fecha a la lista global
        for x_coord, y_coord in coordinates:
            all_coordinates.append({
                'X': x_coord,
                'Y': y_coord, 
                'Date': date_str
            })     
        
        # Crea geometries Point per a cada coordenada de costa
        geometries = [Point(x_coord, y_coord) for x_coord, y_coord in coordinates] #geometries = todos los ptos de costa que encontramos
        
        # Crea GeoDataFrame
        gdf = gpd.GeoDataFrame({
            #'id': range(len(geometries)),  # ID únic per a cada punt, sirve para darle un numero a cada pto
            'x': [coord[0] for coord in coordinates],  # Coordenada X
            'y': [coord[1] for coord in coordinates]   # Coordenada Y
        }, geometry=geometries, crs="EPSG:32631")  # directamente pone las coordenadas en UTM

        # Ruta per al fitxer GeoJSON
        geojson_path = os.path.join(root, "coastline_utm.geojson") #donde lo guardas y crea la ruta
        
        # Guarda com a GeoJSON
        gdf.to_file(geojson_path, driver='GeoJSON') #para guardarlo como un GEOJ
        
        print(f"  → GeoJSON guardat a {geojson_path}")
        print(f"  → S'han exportat {len(coordinates)} punts de costa")

print("\n Processament complet.")


# =====  REPORTE FINAL RMSE =====
print("\n" + "="*60)
print("REPORTE FINAL DE VALIDACIÓN RMSE")
print("="*60)

if validation_results:
    df_validation = pd.DataFrame(validation_results)
    validation_csv_path = os.path.join(base_folder, "validation_results.csv")
    df_validation.to_csv(validation_csv_path, index=False)
    
    print(f"Resultados guardados en: {validation_csv_path}")
    print("\nResumen por fecha:")
    
    dates_with_success = 0
    for result in validation_results:
        status = "✅ CUMPLE" if result['rmse'] < 50 else "❌ NO CUMPLE"
        print(f"  {result['gt_date']}: RMSE = {result['rmse']:.2f}m - {status}")
        if result['rmse'] < 50:
            dates_with_success += 1
    
    print(f"\nTotal fechas validadas: {len(validation_results)}")
    print(f"Fechas con RMSE < 50m: {dates_with_success}")
    
    if dates_with_success >= 3:
        print("🎉 ¡OBJETIVO PRINCIPAL ALCANZADO! RMSE < 50m para al menos 3 fechas")
    else:
        print(f"⚠️  Objetivo no alcanzado. Se necesitan {3 - dates_with_success} fechas más")
else:
    print("❌ No se realizaron validaciones con ground truth")
    print("   Posibles causas:")
    print("   - No hay archivos de ground truth en las rutas especificadas")
    print("   - Las fechas de las imágenes no coinciden con las del ground truth")
    print("   - No se pudieron cargar los archivos GT")

# ===== NUEVO: EJECUTAR ANÁLISIS COMPLETO AL FINAL =====

# 1. Calcular erosión/acreción para 2025
if all_coordinates:
    # Convertir all_coordinates a formato para análisis
    coastline_data = {}
    for coord in all_coordinates:
        date = coord['Date']
        if date not in coastline_data:
            coastline_data[date] = []
        coastline_data[date].append((coord['X'], coord['Y']))
    
    # Ejecutar análisis de erosión/acreción
    erosion_results = calculate_erosion_accretion(coastline_data)
    
    if not erosion_results.empty:
        # Guardar resultados
        erosion_path = os.path.join(base_folder, "erosion_accretion_2025.csv")
        erosion_results.to_csv(erosion_path, index=False)
        print(f"→ Resultados guardados en: {erosion_path}")
        
        # Mostrar resumen
        print("\n📊 RESUMEN EROSIÓN/ACRECIÓN 2025:")
        for _, row in erosion_results.iterrows():
            trend = "📈 ACREACIÓN" if row['net_change_meters'] > 0 else "📉 EROSIÓN"
            print(f"   {row['current_date']}: {trend} ({row['net_change_meters']:+.2f}m)")

# 2. Calcular métricas detalladas de validación
if all_coordinates:
    # Convertir a DataFrame
    df_all_coords = pd.DataFrame(all_coordinates)
    
    # Convertir validation_results a DataFrame
    df_validation = pd.DataFrame(validation_results) if validation_results else pd.DataFrame()
    
    # Ejecutar cálculo de métricas (cloud_ratios es opcional)
    detailed_metrics = calculate_validation_metrics(df_all_coords, df_validation)
    
    # Guardar métricas en CSV
    metrics_path = os.path.join(base_folder, "validation_metrics.csv")
    pd.DataFrame([detailed_metrics]).to_csv(metrics_path, index=False)
    print(f"→ Métricas guardadas en: {metrics_path}")

print("\n🎉 ANÁLISIS COMPLETO FINALIZADO")
print("="*60)

#  GENERAR CSV GLOBAL CON TODAS LAS FECHAS
if all_coordinates:
    # Crear DataFrame con todas las coordenadas
    df_global = pd.DataFrame(all_coordinates)
    
    # Guardar CSV
    csv_path = os.path.join(base_folder, "coastline_coordinates_all_dates.csv")
    df_global.to_csv(csv_path, index=False)
    
    print(f"\n CSV GLOBAL GENERADO:")
    print(f"   → Archivo: {csv_path}")
    print(f"   → Total puntos: {len(df_global)}")
    print(f"   → Fechas únicas: {df_global['Date'].nunique()}")
    print(f"   → Columnas: {list(df_global.columns)}")
    
    # Mostrar primeras filas como ejemplo
    print(f"\n Primeras filas del CSV:")
    print(df_global.head())
else:
    print("⚠️ No se encontraron coordenadas para generar el CSV")
